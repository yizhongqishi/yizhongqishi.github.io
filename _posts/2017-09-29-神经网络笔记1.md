---
layout: post
title: 神经网络笔记1
category: tech
---

#关于BP网络的实现
* 是不是需要实现多隐层: 不需要，隐层单元达到一定数量后由于过拟合的原因只能够增加计算量而不能增加精度



#关于TensorFlow实现LSTM的问题
* 为什么在数据初始化的时候不强制要求用 w[input\_size, hidden\_size] 进行处理：在生成  RNN  网络的函数中是带有这个转换的，因此不需要强制使用
* （MNIST数据处理中）为什么要讲相同一行的数据转换到一起：通过查阅 RNN 网络生成代码我们可以发现，函数会根据我们划分的步长（for input in inputs）来处理函数，结合着<a herf = "http://blog.csdn.net/kkk584520/article/details/51481416">这个链接</a>来看，不难理解，一个 RNN 细胞中包含有多个单元，在大多数的文献和技术博客中，对于 hidden\_size 的解释更多的是 LSTM 单元，我一时真的是转不过来这个弯，我的理解是，一个 RNNCell 就好像一个BP网络的神经元，这就造成了我对于 hidden\_size 有一个极为错误而且矛盾的思路：如果 RNNCell 对应着 BP 网络的一个神经元，那么又改如何理解所有有关 RNN 讲解中为了更好理解 RNN 运行流程而进行的拆分呢？因为在 TensorFlow 实现RNN网络的样例中，对于 hidden\_size 的解释是隐藏层的 LSTM 单元数（更加扯淡的直接说是隐层神经元的数目，对于这种人，我是能说，请自行百度神经元和细胞的关系以及 unit 到底是单元还是神经元）。其实现在想想，在论文中的RNN模型图解是这样的<br/>![RNN图解](../../../../images/lstm.jpg)<br/>我们不难发现，RNN 细胞的输入输出其实是一个序列，外加 LSTM 的公式<br/>![lstm公式](../../../../images/rnn_params.png)<br/>就可以知道每个门的实现其实对应着BP网络中的一层（这里一定要记住，所有的参数都是矩阵）只不过在技术博客的分享中有意无意护忽略了这一点<br/>![lstm图解](../../../../images/lstm.png)<br/>像我这种小白很容易跌落到对于实数的考虑中而不是将矩阵留在心间。另外一点，由于对于 RNN 的讲解是将 RNN 展开的，我承认，这样更加容易理解 RNN 的学习流程到底是什么样的，但是永远不要忘了，隐藏层中一层只有一个 RNN 细胞（一个细胞代表一个隐藏层，听起来是不是有种76的感觉），一个 RNN 细胞中包含一个 LSTM，而LSTM的unit_num是对于 LSTM 各个门的权重 & 偏置矩阵纬度的解释或者说 LSTM 单元才是真正对应着 BP 网络中的神经元。这样一来，对于 RNN 以及 LSTM 结构的理解就较为透彻了（这里我强烈推荐<a href = "http://www.jianshu.com/p/9dc9f41f0b29">这个网站</a>和<a href = "https://www.zybuluo.com/hanbingtao/note/541458">这个网站</a>来学习 LSTM，很受用）