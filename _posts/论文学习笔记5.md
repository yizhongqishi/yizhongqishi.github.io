---
layout: post
title: 论文学习笔记5
category: article
---
本篇笔记主要内容是从强化学习的角度去研究服务器调度问题<br/>
《Using Reinforcement Learning for Autonomic Resource Allocation in Clouds: Towards a Fully Automated Workflow》一篇涉及Q函数加速收敛的论文<br/>
作者设计的奖励函数为：R(s′, a) = CO(a) + PE(s′)其中CO(a)表示执行a操作的花费，PE(s')表示s’状态没有达到理想状态下的惩罚。其中：<br/>
$$
Co(a)=\left\{
\begin{array}{lcl}
c_i * a& & \text{if a > 0} \\
0 & & \text{else}
\end{array}
\right. + c_f * u' * \Delta t
$$
$$
PE(s') = \frac {p_c}{3600} * \Delta t * \left\{
\begin{array}{lcl}
1 + \frac {p' - P_{SLA}} {P_{SLA}} & &\text{if $p' > P_{SLA}$}\\
0 &&\text{else}
\end{array}
\right.
$$

* $c_f$为节点运行费用
* $c_i$代表部署一个节点需要的固定花费，为$\frac{c_f}{60}$
* $\Delta t$为两次决定之间的时间间隔
* $p_c$为节点不满足性能要求的时候给出的惩罚，这里以SLA给出的惩罚为主，作者给出的单位为每小时，估计是SLA中的要求。

不难看出，这些值实际上会影响学习模型的效果的。相对于普通Q函数每次都是从一个状态跳到另一个状态的更新方式，作者提出类似值函数迭代的方式，每次值更新都将所有的状态遍历一遍，以此加速。为了使用值迭代的方式加速，<b>作者通过统计的方式估计出转移状态函数和奖励函数等</b>。通过作者的试验，我们可以看出，训练初期使用加速会导致误差变大，但是随着训练次数的增加，在中期（3万次训练内）却更快达到稳定的状态。但是3万次后两者的差异几乎没有。通过统计状态转移函数和奖励函数，能够看出两个新旧模型之间的差异，从而探测出应用行为的变化。但是这种行为代价太大了，主要是要从原来的免模型变为类似模型已知，这样才能使用值迭代的方式。<br/>

---

《Autoscaling for Hadoop Clusters》主要介绍了有关在Hadoop中Map阶段和Reduce阶段工作时间预测的模型，从而弹性伸缩控制
