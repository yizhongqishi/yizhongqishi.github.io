---
layout: post
title: 论文学习笔记6
category: article
---
《A Comparison of Reinforcement Learning Techniques for Fuzzy Cloud Auto-Scaling》模糊论和强化学习的结合<br/>
模糊逻辑使我们能够将规则形式的专家知识转化，在给定的情况下应用它，并根据专家知识得出一个合适的最佳行为。模糊规则是IF-THEN规则的集合，代表人类对如何进行决策和控制目标系统的知识。FRL通过不断监测应用的不同特征（例如，工作负载和响应时间），验证系统目标的满意度并调整资源分配以维持目标满意度，从而遵循自主MAPE-K循环。<br/>
需要学习的特征：工作负载(?体现在哪些方面）、响应时间、虚拟机数量。这里强化学习生成的是规则，也就是说提供模拟规则库。<br/>
（a）可预测的爆发模式,表示受季节性趋势或高性能计算服务的典型峰值和谷值的工作负载类型，（b）变化模式,反映了新闻和媒体，事件登记或快速销售等应用，（c）ON＆OFF模式,反映了分析，银行/税务机构和测试环境等应用。<br/>
在基本的强化学习中，从一个状态到另一个状态的转换是经过一次确定的行动的，但是模糊控制中则是经过规则集合的过滤后产生的最终决策。比如，经过第一个策略（CPU负载低），服务器减一，但是经过第二个策略（用户请求量变大）服务器又加二，则最终的决策是加一。因此使用模糊论进行强化学习的优化时是将所有状态涉及的策略添加进一个执行权重（可能性）来判断出最终的行动。这种情况让决策的制定更加科学以及人性化，作者认为当Q函数不再变化时，算法实现了收敛，但作者并没有对这两种强化学习方法进行时间和资源消耗上进行评估。作者尝试使用历史数据进行q表的初始化，在一定程度上减少了算法学习的时间，但是或许在q表的规模上能够进一步优化来减少资源消耗。作者的实验表明，对于可预测的波动，SL方法更有效果，因为减少了对于新策略的探索，而QL在不规律变化中更好，因为带有了新规则的探索。这个在算法的实现中比较有用。

---

有关diss阈值方法：这种方法有几个缺点。首先，弹性规则必须由定量值精确确定，这需要深入的知识和专业知识。此外，现有的基于阈值的方法没有明确地处理基于云的软件中的不确定性，其中噪声和意外事件是常见的。而且规则中需要准确定义阈值，比如我们设定SLA的60%为阈值，但是实际上可能从60%到100%只需要5分钟的时间而我们的部署要10分钟，那么我们的这个设定就收效很小。我们需要一种能够根据已有的数据预测的方式来尽可能减少当前主观对未来情况不可知所造成的影响。还有就是比如响应时间的反馈上可能出现噪声引起数值上的波动，这样可能会触发资源调度方法。而实际上并不需要。

---
《Autonomic Resource Provisioning for Cloud-Based Software》使用模糊论实现弹性伸缩<br/>
当我们在谈论弹性伸缩的时候，我们在谈论：

* 可扩展性，系统承受工作负载波动的能力，
* 成本效率，只释放未使用资源 
* 时间效率，一旦提出请求就获取和释放资源。

在由于每个调度策略都需要一定的时间才能够充分发挥作用，因此在实现基于阈值的调度方法时每次的策略更改一定要跟着一个冷却值来保证探测器能够反馈到正确的结果。而且添加节点和删除节点的冷却值不一样。<br/>
采用模糊论实现控制器的好处在于比较好地接近人类社会中相对模糊而不是准确的信息交流，我们实际上对于弹性控制器的想法可能是“当请求比较多或者是当服务器的响应时间又或者是服务器的处理性能都已经不能够满足/过剩于当前的情况时候，进行“一些”服务器的调度。和阈值控制相比，虽然也是需要建立一个调度方案库，但是并不要求设定确切的值，本身在调度选择上就体现出了弹性。<br/>
作者有关二型模糊集的建立上采用的是专家经验，通过专家意见的平均值和标准差作为上级隶属函数和下级隶属函数的参数。采用二型模糊集实际上是因为考虑到隶属函数本身都是模糊的，这种情况下使用次级隶属函数来将隶属函数本身模糊化。<br/>
在预测上，作者使用二次指数平滑法进行数值的预测，从而显示出数值变化的趋势；对于服务器响应时间使用一次指数平滑来估计。在资源的分配上，作者也使用了优先级更高的约束策略，来防止模糊决策造成过高的成本或是危及用户体验。弹性调度的起点在应用层，然后才去考虑数据层。<br/>
单点模糊集有点像one-hot，转换为度量值时直接选择最大隶属度的描述（具体来说就是便利所有度量的隶属度，然后并操作），然后进行接下来的模糊控制<br/>
本文好处在于通过引进模糊论和时间分析，能够有效提升在弹性伸缩上的效果，而且需要调整的参数小降低了模糊控制器的实现难度，但是需要注意的是，模糊知识库是建立在专家系统上的，这也就表明人工在本方法中仍然是出于较为重要的地位。是否能够将模糊论引入到强化学习参数初始化阶段，从而让先验知识应用到强化学习中，加速初始阶段的学习，甚至是改善强化学习阶段中模型不可用的状态。实际上，之前在Q函数加速论文中，通过值迭代的方式已经能够改善学习中模型效果差的情况，也就是通过每次对所有状态的遍历对模型的学习方向施加一个约束，模糊模型是否也能够实现约束的作用呢？

---

《A Hybrid Reinforcement Learning Approach to Autonomic Resource Allocation》排队模型和强化学习结合的混合弹性伸缩模型

（i）在没有领域知识或好的启发式的情况下，最初的RL状态可能对应于任意错误的初始策略; （ii）一般来说，RL程序也需要包括一定程度的“探索”，认为这些行为是不理想的。典型的探索机制涉及随机行为选择，并且在现场系统中实施可能会非常昂贵。<br/>

在文中，作者为了降低增强学习的状态空间，将原有的学习值（状态、行为）转变为（请求到达速率，分配机器数目），这样就能够通过排队论MM1预测出来响应时间，但是这样的问题是，对于具有扩展型属性要求的应用就可能有不好的效果。而且在策略的选择上，只有新策略更优，才会发生策略的更改。

《Empirical Prediction Models for Adaptive Resource Provisioning in the Cloud》通过神经网络和滑动窗口算法进行弹性调度，2011年的，比较老，但是思路可以
